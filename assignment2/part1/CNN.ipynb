{"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"PwNpC5conPMS"},"source":["# NumPy Based CNN block (13 points)"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Nud1FqU0nPMX"},"source":["##   Outline of the Assignment\n","\n","You will be implementing the building blocks of a convolutional neural network\n","\n","1. **`zero padding`**\n","\n","2. **`convolution : Forward`**\n","\n","3. **`convolution : Backward`**\n","\n","4. **`Max pooling : Forward`**\n","\n","5. **`Max pooling : Backward`**\n","\n","\n","    \n","This notebook will ask you to implement these functions from scratch in **`Numpy`**.\n","\n","\n","**Note** that for every forward function, there is its corresponding backward equivalent. "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"colab_type":"code","id":"mlGba2SdnPMZ"},"outputs":[],"source":["import torch\n","import numpy as np\n","import matplotlib.pyplot as plt"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"NVLBNfwjnPMh"},"source":["## 1. Zero Padding (1 pts.)"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"uIofak2KnPMk"},"source":["Zero-padding adds zeros around the border of an image:\n","\n","**Exercise**  : Implement the following function, which pads all the images of a batch of examples X with zeros."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"3Xac07WonPMn"},"source":["shape of X and its zero pad array is :\n","\n","\n","$$ X : (N, C, i_h, i_w)   $$\n","$$  \\text{zeropad}(X) : (N, C, i_h + 2*ph, i_w + 2*pw)$$"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"xYa5kwC7nPMp"},"source":["**Note** : you should not use np.pad in your implementation"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"colab_type":"code","id":"-2rbQl4enPMr"},"outputs":[],"source":["def zero_padding(X, padding):\n","    \"\"\"\n","    Pad with zeros all images of the dataset X. The padding is applied to the height and width of an image.\n","\n","    input :\n","      - X :  numpy array of shape (N, C, IH, IW) representing a batch of N images\n","      - padding : a tuple of 2 integer (ph, pw), amount of padding around each image on vertical and horizontal dimensions\n","    return :\n","      - zero_pad : zero pad array of shape (N, C, IH + 2*ph, IW + 2*pw)\n","    \n","    \"\"\"\n","  \n","    zero_pad = None\n","    ###########################################################################\n","    # Hint: you should not use the function np.pad for padding.                     \n","    ###########################################################################\n","    \n","    pass\n","    ###########################################################################\n","    #                             END OF YOUR CODE                            #\n","    ###########################################################################\n","    \n","    return zero_pad\n","    \n","    \n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"colab_type":"code","id":"bOvCLShTnPMy"},"outputs":[],"source":["#test zero_padding function\n","np.random.seed(1968)\n","\n","x = np.random.rand(2, 3 ,4, 4)\n","padding = (3, 2)\n","x_pad = zero_padding(x, padding)\n","\n","\n","assert x_pad.shape==(x.shape[0], x.shape[1], x.shape[2] + 2*padding[0], x.shape[3] + 2*padding[1])\n","assert np.all(x_pad[:, :, padding[0]:padding[0]+x.shape[2], padding[1]:padding[1]+x.shape[3]]==x)\n","\n","print(\"your implementation is correct\")\n","print(\"shape of x is :\", x.shape)\n","print(\"shape of x_pad is :\", x_pad.shape)\n","\n","fig, axarr = plt.subplots(1, 2)\n","axarr[0].set_title('x')\n","axarr[0].imshow(x[0, 0, :, :])\n","axarr[1].set_title('x_pad')\n","axarr[1].imshow(x_pad[0, 0, :, :])"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"yaLgNcJonPM5"},"source":["## 2.convolution : Forward (2 pts.)"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"iSHkDYrfnPM7"},"source":["In this Exercise, you implement convolutional neural networks using the NumPy library only.\n","\n","The input X,W are the input of the convolutional layer and the shape of X,W are $(N, C, i_h, i_w)$ , $(F, C, f_h, f_w)$ respectively and The return  value O is the output of the convolutional layer and the shape is $(N, F, O_h, O_w)$ where :\n","\n","$$\\text{stride} : (s_h,s_w)$$\n","\n","$$\\text{padding} : (p_h,p_w)$$\n","\n","$$O_w =\\lfloor \\frac{i_w - f_w + 2*p_w}{s_w} \\rfloor + 1$$\n","\n","$$O_h = \\lfloor\\frac{i_h - f_h + 2*p_h}{s_h}\\rfloor + 1$$\n","$$O(b,f, i ,j)=\\sum_{r=0}^{C-1}\\sum_{k=0}^{f_h-1}\\sum_{l=0}^{f_w-1} W(f,r,k,l) X(b,r,s_h *i +k, s_w  *j +l)$$"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"colab_type":"code","id":"rc6Tt8EGnPM9"},"outputs":[],"source":["def convolution2D(X, W, stride, padding):\n","    \"\"\"\n","    A implementation of the forward pass for a convolutional layer.\n","    \n","    The input consists of N data points, each with C channels, height IH and\n","    width IW .We convolve each input with F different filters, where each filter\n","    spans all C channels and has height FH and width FW.\n","    \n","    \n","    inputs:\n","     - X : input data of shape (N, C, IH, IW)\n","     - W : Filter weight of shape (F, C, FH, FW)\n","     - stride : a tuple of 2 integer (sh, sw)\n","     - padding :a tuple of 2 integer (ph, pw)\n","     \n","    return:\n","     - out : Output data, of shape (N, F, OH, OW) where OH and OW given by\n","     \n","     OH= 1 + int ( (IH + 2*ph - FH)/ sh )\n","     OW= 1 + int ( (IW + 2*pw - FW)/ sw )\n","    \n","    \"\"\"\n","    \n","    out = None\n","    ###########################################################################\n","    # Implement the convolutional forward pass.                               #\n","    ###########################################################################\n","    pass\n","    ###########################################################################\n","    #                             END OF YOUR CODE                            #\n","    ###########################################################################\n","    \n","    return out\n","    \n","    \n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"kuqhD5E8nPNJ"},"source":["To test your implementation, we will compare the results  with torch function (torch.nn.functional.conv2d)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"colab_type":"code","id":"_Ran5YbunPNM"},"outputs":[],"source":["np.random.seed(1973)\n","param1 = {'X':np.random.rand(2, 3, 23, 20), 'W':np.random.rand(7, 3, 6, 6), 'stride':(3, 6), 'padding':(2, 3)}\n","\n","w_t = torch.from_numpy(param1['W']).float()\n","x_t = torch.from_numpy(np.pad(param1['X'], ((0, 0), (0, 0), (2, 2), (3, 3)), 'constant', constant_values=0)).float()\n","conv = torch.nn.functional.conv2d(x_t, w_t, stride=param1['stride'], padding='valid')\n","conv = conv.cpu().detach().numpy()\n","\n","conv_numpy = convolution2D(**param1)\n","\n","assert conv.shape==conv_numpy.shape\n","print(\"Error :\", (np.sum(conv - conv_numpy)**2))\n","print(\"output shape :\", conv_numpy.shape)\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Yam8Y2x8nPNe"},"source":["** Expected Output: **\n","<table>\n","    <tr>\n","        <td>\n","            **out shape**\n","        </td>\n","        <td>\n","            (2, 7, 8, 4)\n","        </td>\n","    </tr>\n","    <tr>\n","        <td>\n","            **ERROR**\n","        </td>\n","        <td>\n","            2.5559093329160782e-28\n","       </td>\n","    </tr>\n","    \n","</table>\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"fRFXIZfwnPNg"},"source":["## 3.convolution : Backward"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"aEf-K0MKnPNi"},"source":["### 3.1 - Backward  w.r.t. filter (2 pts.)"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"1guVkIsfnPNk"},"source":["This is the formula for computing a $\\frac{\\partial L}{\\partial W}$ for a single $W(f^\\prime,c^\\prime,k^\\prime ,l^\\prime )$ that $W$ is 4-D array as a filter in convolution operation with shape $(F,C,f_h,f_w)$ "]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"LmJRQzNRnPNm"},"source":["$$\\frac{\\partial L}{\\partial W(f^\\prime,c^\\prime,k^\\prime ,l^\\prime )} = \\sum_{b=0}^{N-1}\\left (\\sum_{i=0}^{O_h-1}\\sum_{j=0}^{O_w-1} \\frac{\\partial L}{\\partial O(b,f^\\prime,i,j)} \\frac{\\partial O(i,j)}{\\partial W(f^\\prime,c^\\prime,k^\\prime ,l^\\prime )}\\right ) = \\sum_{b=0}^{N-1}\\left (\\sum_{i=0}^{O_w-1}\\sum_{j=0}^{O_h-1} \\frac{\\partial L}{\\partial O(b,f^\\prime,i,j)}  X(b,c^\\prime, s_h*i +k^\\prime, s_w*j +l^\\prime) \\right )$$\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"colab_type":"code","id":"g2g-PgNmnPNo"},"outputs":[],"source":["def convolution2D_backward_filter(out_grad, X, W, stride):\n","    \"\"\"\n","    A implementation of the backward pass for a convolutional layer.\n","    \n","    inputs:\n","     - out_grad  : gradient of the Loss with respect to the output of the conv layer with shape (N, F, OW, OH)\n","     - X : input data of shape (N, C, IH, IW)\n","     - W : Filter weight of shape (F, C, FH, FW)\n","     - stride : a list of [sh, sw]\n","     \n","    return:\n","     - dW : Gradient with respect to W\n","    \n","    \"\"\"\n","    dW = None\n","    ###########################################################################\n","    # Implement the convolutional backward pass.                              #\n","    ###########################################################################\n","    pass\n","    ###########################################################################\n","    #                             END OF YOUR CODE                            #\n","    ###########################################################################\n","    \n","    return dW\n","    \n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"colab_type":"code","id":"N5GrLdnlnPNu"},"outputs":[],"source":["np.random.seed(1345)\n","\n","param = {'X':np.random.rand(2, 3, 10,10), 'W':np.random.rand(7, 3, 4, 4), 'stride':(2, 2)}\n","c_1 = np.ones((2, 7, 4, 4))   \n","dw = convolution2D_backward_filter(c_1, **param)\n","w_t = torch.from_numpy(param['W']).float()\n","x_t = torch.from_numpy(param['X']).float()\n","x_t.requires_grad = True\n","w_t.requires_grad = True\n","c = torch.nn.functional.conv2d(x_t, w_t, stride=param['stride'], padding='valid')\n","\n","loss = c.sum()\n","loss.backward()\n","dw_t = w_t.grad.cpu().detach().numpy()\n","\n","\n","print(\"Error  :\", np.sum((dw-dw_t)**2))\n","print(\"dW_t  :\", np.sum(dw_t))\n","print(\"dW  :\", np.sum(dw))"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"FzXtSW_InPN0"},"source":["** Expected Output: **\n","<table>\n","    <tr>\n","        <td>\n","            **dW_tf**\n","        </td>\n","        <td>\n","            5340.576411697173\n","        </td>\n","    </tr>\n","    <tr>\n","        <td>\n","            **dW**\n","        </td>\n","        <td>\n","            5340.576411697173\n","        </td>\n","    </tr>\n","    <tr>\n","        <td>\n","            **Error**\n","        </td>\n","        <td>\n","            2.473867798773093e-27\n"," </td>\n","    </tr>\n","\n","</table>\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"nxyz9o2GnPN3"},"source":["### 3.2 - Backward  w.r.t. input (2 pts.)"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"H7P5oyWXnPN5"},"source":["This is the formula for computing a $\\frac{\\partial L}{\\partial X}$ for a single $X(b^\\prime,c^\\prime,k^\\prime ,l^\\prime )$ that $X$ is 4-D array as a input in convolution operation with shape $(N,C,i_h,i_w)$ "]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"nLGji2fKnPN7"},"source":["$$\\frac{\\partial L}{\\partial X(b^\\prime,c^\\prime,k^\\prime ,l^\\prime )} = \\sum_{f=0}^{F-1}\\left (\\sum_{i=0}^{O_h-1}\\sum_{j=0}^{O_w-1} \\frac{\\partial L}{\\partial O(b^\\prime,f,i,j)} \\frac{\\partial O(b^\\prime,f,i,j)}{\\partial X(b^\\prime,c^\\prime,k^\\prime ,l^\\prime )}\\right ) = \\sum_{f=0}^{F-1}\\left (\\sum_{i=0}^{O_h-1}\\sum_{j=0}^{O_w-1} \\frac{\\partial L}{\\partial O(b^\\prime,f,i,j)} W(f,c^\\prime,k^\\prime - s_h*i, l^\\prime - s_w*j) \\right )$$\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"colab_type":"code","id":"k9-Ez6OQnPN-"},"outputs":[],"source":["def convolution2D_backward_input(out_grad, X, W, stride):\n","    \"\"\"\n","    A implementation of the backward pass for a convolutional layer.\n","    \n","    inputs:\n","     - out_grad  : gradient of the Loss with respect to the output of the conv layer with shape (N, F, OW, OH)\n","     - X : input data of shape (N, C, IH, IW)\n","     - W : Filter weight of shape (F, C, FH, FW)\n","     - stride : a list of [sh, sw]\n","     \n","    return:\n","     - dX : Gradient with respect to X\n","    \n","    \"\"\"\n","        \n","    dX = None\n","    ###########################################################################\n","    # Implement the convolutional backward pass.                              #\n","    ###########################################################################\n","    pass\n","    ###########################################################################\n","    #                             END OF YOUR CODE                            #\n","    ###########################################################################\n","    \n","    return dX\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"colab_type":"code","id":"VJwwMZqInPOE"},"outputs":[],"source":["np.random.seed(1992)\n","\n","param = {'X':np.random.rand(5, 3, 6, 6), 'W':np.random.rand(2, 3, 2, 2), 'stride':(3,3)}\n","grad = np.ones((5, 2, 2, 2))\n","dx = convolution2D_backward_input(grad, **param)\n","\n","\n","\n","w_t = torch.from_numpy(param['W']).float()\n","x_t = torch.from_numpy(param['X']).float()\n","x_t.requires_grad = True\n","w_t.requires_grad = True\n","c = torch.nn.functional.conv2d(x_t, w_t, stride=param['stride'], padding='valid')\n","\n","loss = c.sum()\n","loss.backward()\n","dx_t = x_t.grad.cpu().detach().numpy()\n","\n","\n","\n","assert dx.shape==dx_t.shape\n","print(\"Error is :\", np.sum((dx-dx_t)**2))\n","print(\"dX_tf is :\", np.sum(dx_t))\n","print(\"dX is :\", np.sum(dx))"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"kmIIdIwqnPOL"},"source":["** Expected Output: **\n","<table>\n","    <tr>\n","        <td>\n","            **dX_tf**\n","        </td>\n","        <td>\n","            208.39287018595633\n","        </td>\n","    </tr>\n","    <tr>\n","        <td>\n","            **dX**\n","        </td>\n","        <td>\n","            208.39287018595633\n","        </td>\n","    </tr>\n","    <tr>\n","        <td>\n","            **Error**\n","        </td>\n","        <td>\n","            0.0\n"," </td>\n","    </tr>\n","\n","</table>\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"01DiIHblnPOO"},"source":["## 4.Pooling"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"0s_-ssYznPOQ"},"source":["### 4.1 - forward max pooling (1 pts.)"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Zuq2YNg1nPOS"},"source":["The pooling layer reduces the height and width of the input. It helps reduce computation"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"f_SFsAmLnPOV"},"source":[" - Max-pooling layer: slides an ($f, f$) window with stride $s$ over the input and stores the max value of the window in the output.\n","\n","in function below X is input and shape of X is $(N, C, i_h, i_w)$  and output is shape $(N, C, O_h, O_w)$ that :\n","\n"," $$O_h =\\lfloor\\frac{i_h - f }{s}\\rfloor + 1$$\n"," $$O_w =\\lfloor\\frac{i_w - f }{s}\\rfloor + 1$$"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"GEYR4RCNnPOX"},"source":["**Exercise**: Implement the forward pass of the pooling layer\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"colab_type":"code","id":"VaWcSaPKnPOZ"},"outputs":[],"source":["def pool_forward(X, f, s):\n","    \"\"\"\n","    Implements the forward pass of the pooling layer\n","    \n","    input:\n","       - X : numpy array of shape (N, C, IH, IW)\n","       - f : int, filter size in height and width dim\n","       - s : int\n","    \n","    Returns:\n","       - pool : output of the pool layer, a numpy array of shape (N, C, OH, OW) where OH and OW given by\n","       \n","       OH = 1 + int((IH - f)/s)\n","       OW = 1 + int((IW - f)/s)\n","    \n","    \"\"\"\n","    pool = None\n","    ###########################################################################\n","    #  Implement the max pooling forward pass.                                #\n","    ###########################################################################\n","    pass\n","    ###########################################################################\n","    #                             END OF YOUR CODE                            #\n","    ###########################################################################\n","    \n","    return pool\n","    \n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"colab_type":"code","id":"JtN8FqAlnPOj"},"outputs":[],"source":["np.random.seed(1975)\n","x=np.random.rand(2,3,23,23)\n","\n","hyper_param={\"f\":2, \"s\" :11}\n","c=pool_forward(x,**hyper_param)\n","\n","pooling = torch.nn.MaxPool2d(2, 11)\n","\n","x_t = torch.from_numpy(x)\n","x_t.requires_grad = True\n","pool_t = pooling(x_t).cpu().detach().numpy()\n","\n","assert c.shape==pool_t.shape\n","assert (c==pool_t).all()\n","\n","print(\"your implementation is correct\")\n","print(\"output shape :\", c.shape)\n","print(\"output :\", c)\n","print(\"Error :\" ,np.sum((c-pool_t)**2))\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"hr0tWI2qnPOp"},"source":["** Expected Output: **\n","<table>\n","    <tr>\n","        <td>\n","            **Error**\n","        </td>\n","        <td>\n","            0.0\n","        </td>\n","    </tr>\n","    <tr>\n","        <td>\n","            **output shape **\n","        </td>\n","        <td>\n","            (2, 3, 2, 2)\n","        </td>\n","    </tr>\n","\n","\n","</table>\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"SRCZdRWinPOr"},"source":["### 4.2 - backward max pooling (1 pts.)"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"FN1kDBVhnPOt"},"source":["**Exercise**  :implement the backward pass for the pooling layer. Even though a pooling layer has no parameters for backprop to update, you still need to backpropagation the gradient through the pooling layer in order to compute gradients for layers that came before the pooling layer. \n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"colab_type":"code","id":"hpSvOKFWnPOy"},"outputs":[],"source":["def pool_back(grad_pool, X, f, s):\n","    \"\"\"\n","    Implements the backward pass of the pooling layer\n","    \n","    Arguments:\n","       - grad_pool : gradient of cost with respect to the output of the pooling layer\n","       - X : input to pooling layer , numpy array with shape (N, C, IH, IW)\n","       - f : int, filter size in height and width dim\n","       - s :  int\n","    Returns:\n","       - dX_pool  : gradient of cost with respect to the input of the pooling layer, same shape as X\n","    \"\"\"\n","    dX_pool = None\n","    ###########################################################################\n","    # Implement the max pooling backward pass.                               #\n","    ###########################################################################\n","    pass\n","    ###########################################################################\n","    #                             END OF YOUR CODE                            #\n","    ###########################################################################\n","    \n","    return dX_pool\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"colab_type":"code","id":"1uBU0WeUnPO3"},"outputs":[],"source":["np.random.seed(19)\n","X=np.random.rand(2,3,10,10)\n","g=np.random.rand(2,3,7,7)\n","\n","f=2\n","s=1\n","dX=pool_back(g, X, f, s)\n","\n","\n","print(\"mean of dX :\",np.mean(dX))\n","print(\"dX[1,2,2:5,2:5] = \",dX[1,2,2:5,2:5])\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"0qOjFOKonPO-"},"source":["**Expected Output**: \n","\n","<table> \n","<tr> \n","<td>\n","\n","**mean of dX =**\n","</td>\n","\n","<td>\n","\n","0.2428232587752177\n","  </td>\n","</tr>\n","\n","<tr> \n","<td>\n","**dX[1,2,2:5,2:5] =** \n","</td>\n","<td>\n","[[0.         0.         1.09075724]\n"," [1.29276074 0.         1.15881724]\n"," [0.         0.61727634 0.        ]]\n","</td>\n","</tr>\n","</table>\n","\n"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"CNN_modify.ipynb","provenance":[],"version":"0.3.2"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":0}
